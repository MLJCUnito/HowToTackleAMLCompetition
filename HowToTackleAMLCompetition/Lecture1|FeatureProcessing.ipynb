{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Lecture1|FeatureProcessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLJCUnito/ProjectX2020/blob/master/HowToTackleAMLCompetition/Lecture1%7CFeatureProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SFVvbTdQHl4",
        "colab_type": "text"
      },
      "source": [
        "# **Lecture 1: Feature Processing**\n",
        "(Author: Simone Azeglio, simone.azeglio@edu.unito.it)\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "## **Overview**\n",
        "\n",
        "* [Practical Gradient Descent](#section1)\n",
        "    * [Macro, Meso, Micro-scale in Science](#section1.1)\n",
        "    * [Scaling in Data Science](#section1.2)\n",
        "    * [Preprocessing Data](#section1.3)\n",
        "\n",
        "* [Feature Engineering](#section2)\n",
        "    * [Feature Importance](#section2.0)\n",
        "    * [Feature Extraction](#section2.1)\n",
        "    * [Feature Selection](#section2.2)\n",
        "    * [Feature Construction](#section2.3)\n",
        "\n",
        "* [Discerning between different kinds of features](#section3)\n",
        "\n",
        "* [Handling missing values](#section4)\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "<a id='section1'></a>\n",
        "## **Practical Gradient Descent**\n",
        "In this first part, we'd like to tell you about some practical tricks for making **gradient descent** work well, in particular, we're going to delve into feature scaling. As an introductory view, it seems reasonable to try to depict an intuition of the concept of *scale*. \n",
        "\n",
        "<a id='section1.1'></a>\n",
        "### **Macro, Meso, Micro-scale in Science**\n",
        "\n",
        "As scientists, we are well aware of the effects of using a specific measurement tool in order to characterize some quantity and describe reality. As an ideal example we consider the **length scale**. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sazio/sazio.github.io/master/_posts/img/HowToTackle/Lecture1/1.0.png\" width=\"500\" height=\"300\">\n",
        "\n",
        "We can identify three different points of view: *microscopic*, *mesoscopic* and *macroscopic*; which are intimately related to the adopted lenght scale. \n",
        "\n",
        "We usually deal with the *macroscopic scale* when the observer is in such a position (pretty far, in terms of distance), with respect to the object, that she/he can describe its global characteristics. Instead, we do refer to the *microscopic scale* when the observer is so close to the object that she/he can describe its atomistic details or elementary parts (e.g. molecules, atoms, quarks). Last but not least, we talk about *mesoscopic scale* everytime we are in between micro and macro. \n",
        "\n",
        "These definitions are deliberately vague, since delineating a precise and neat explanation would be higly difficult and complex, and it's actually far from our purposes. \n",
        "\n",
        "On the other side, this kind of introduction is quite useful, we should take a few minutes to think about the \"active\" role of the observer and about the fact that, to be honest, for every length scale, there's some specific theory, i.e. there's no global theory for a multi-scale description of some phenomenon. \n",
        "\n",
        "<a id='section1.2'></a>\n",
        "### **Scaling in Data Science**\n",
        "\n",
        "If our beloved observer (i.e. the scientist) has some kind of \"privilege\", i.e. choosing the right measurement tool, which is nothing but choosing the right scale in the description of some phenomenon, we can't really say the same for a data scientist. \n",
        "\n",
        "It's a sort of paradox, but a data scientist can't really deal with data retrieval most of the times. Because of that, a data scientist is often left alone in front of data, without even knowing from which measurement tool they're coming from. There's no way to interact with the length scale for example. \n",
        "\n",
        "Is there something that we can do about it? The only thing we can do is assuming that features are independent and scale these features in order to have something compatible from one to the other. This procedure is called **feature scaling**, and soon we'll understand why it is useful even for ML algorithms, such as gradient descent. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sazio/sazio.github.io/master/_posts/img/HowToTackle/Lecture1/1.1.png\" width=\"500\" height=\"300\">\n",
        "\n",
        "If you make sure that features are on similar scales, i.e. features take on similar range of values, then gradient descent can converge more quickly. \n",
        "\n",
        "More concretely, let's say we have a problem with two features where $x_1$ is the length of a football field and take values between $90$ (meters) and $115$ (meters) and $x_2$ is the radius of a ball which takes values between $10.5* 10^{-2}$ (meters) to $11.5* 10^{-2}$ (meters). If you plot the countours of the cost function $J(\\omega)$ then you might get something similar to the *left plot*, and because of these very skewed elliptical shape, if we run gradient descent on this cost function, it may end up taking a long time and oscillating back and forth before reaching the global minimum. \n",
        "\n",
        "In these settings, as stated previously, a useful thing to do is to scale the features. Generally, the idea is to get every feature into approximately a $-1$ to $+1$ range. By doing this, we get the *right plot*. In this way, you can find a much more direct path to the global minimum rather than taking a much more convoluted path where you're sort of trying to follow a very complicated trajectory. \n",
        "\n",
        "<a id='section1.3'></a>\n",
        "### **Preprocessing Data**\n",
        "ToDo: \n",
        "\n",
        "CODE\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "<a id='section2'></a>\n",
        "## **Feature Engineering**\n",
        "\n",
        "The real deal is that nobody explicitly tells you what **feature engineering** is, in some way, you are expected to understand for yourself what are good features.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sazio/sazio.github.io/master/_posts/img/HowToTackle/Lecture1/1.2.jpg\" width=\"700\" height=\"300\">\n",
        "\n",
        "> Feature engineering is another topic which doesn’t seem to merit any review papers or books, or even chapters in books, but it is absolutely vital to ML success. […] Much of the success of machine learning is actually success in engineering features that a learner can understand. \n",
        "\n",
        ">*(Scott Locklin, in “Neglected machine learning ideas”)* \n",
        "\n",
        "Let's try to figure out what feature engineering is. \n",
        "\n",
        "In solving such problems, our goal is to get the best possible result from a model. In order to achieve that, we need to extract useful information and get the most from what we have. On one side, this includes getting the best possible result from the algorithms we are employing. On the other side, it also involves getting the most out of the available data. \n",
        "\n",
        "*How do we get the most out of our data for predictive modeling?* \n",
        "\n",
        "Feature engineering tries to find an answer to this question. \n",
        "\n",
        "> Actually, the success of all Machine Learning algorithms depends on how you present the data. \n",
        "\n",
        "> (*Mohammad Pezeshki, answer to “What are some general tips on feature selection and engineering that every data scientist should know?\"*)\n",
        "\n",
        "\n",
        "\n",
        "<a id='section2.0'></a>\n",
        "### **Feature Importance**\n",
        "Feature importance refers to a bunch of techniques that assign a score to input features based on how useful they are at predicting a target variable. These scores play an important role in predictive modeling, they usually provide useful insights into the dataset and the basis for dimensionality reduction and feature selection. \n",
        "\n",
        "Feature importance scores can be calculated both for regression and classification problems. \n",
        "\n",
        "These scores can be used in a range of situations, such as:\n",
        "* *Better understanding the data*: the relative scorse can highlight which features may be most relevant to the target, and on the other side, which are least relevant. This could be a useful notion for a domain expert and could be used as a basis for gathering more or different data. \n",
        "\n",
        "\n",
        "* *Better understanding a model*: inspecting the importance score provides insights into the specific model we're using and which features are the most important to the model when elaborating a prediction. \n",
        "\n",
        "\n",
        "* *Reducing the number of input features*: we can use the importance scores to select those features to delete (lowest score) and those to keep (highest scores). \n",
        "\n",
        "Now let's jot down a few lines of code in order to grasp this topic in a better way. \n",
        "\n",
        "---\n",
        "\n",
        "**<ins> Check Scikit-Learn Version </ins>**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "089FvEc8QHl5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9ea577c9-83c3-4e85-88a8-680c4a33271a"
      },
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.22.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFmSLA3SRUhn",
        "colab_type": "text"
      },
      "source": [
        "Now, in order to explore feature importance scores, we'll import a few test datasets directly from sklearn. \n",
        "\n",
        "**<ins>Classification Dataset</ins>**\n",
        "\n",
        "Easy peasy, we can use the [*make_classification()*](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function to create a test binary classification dataset. \n",
        "\n",
        "We can specify the number of samples and the number of features, some of them are going to be informative and the remaining redundant. (*Tip*: you should fix the *random seed*, in this way you'll get a reproducible result)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CchIfeKMSO_g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a662a254-bf1d-43bd-9ef9-b818a524fc6a"
      },
      "source": [
        "# classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "# define dataset\n",
        "X_clf, y_clf = make_classification(n_samples=1000, n_features=15, n_informative=6, n_redundant=9, random_state=1)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 15) (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9ZoNTa1SOVV",
        "colab_type": "text"
      },
      "source": [
        "**<ins>Regression Dataset</ins>**\n",
        "\n",
        "In a parallel fashion, we'll use the [*make_regression()*](https://machinelearningmastery.com/calculate-feature-importance-with-python/#:~:text=Feature%20importance%20refers%20to%20techniques,at%20predicting%20a%20target%20variable.) function to create a regression dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlC9R4MOTkYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59ad0529-116f-4168-f64a-35c846cc4d5a"
      },
      "source": [
        "# test regression dataset\n",
        "from sklearn.datasets import make_regression\n",
        "# define dataset\n",
        "X_reg, y_reg = make_regression(n_samples=1000, n_features=15, n_informative=6, random_state=1)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 15) (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ9hLi-7VZim",
        "colab_type": "text"
      },
      "source": [
        "### Coefficients as Feature Importance \n",
        "\n",
        "When we think about linear machine learning algorithms, we always fit a model where the prediction is the weighted sum of the input values (e.g. linear regression, logistic regression, ridge regression etc..) \n",
        "\n",
        "These coefficients can be used directly as naive feature importance scores. Firstly we'll fit a model on the dataset to find the coefficients, then summarize the importance scores for each input feature and create a bar chart to get an idea of the relative importance. \n",
        "\n",
        "<ins>**Linear Regression Feature Importance**</ins>\n",
        "\n",
        "It's time to fit a [*LinearRegression()*](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model on the regression dataset and get the *coef_* property that conatins the coefficients. The only assumption is that the input variables have the same scale or have been scaled prior to fitting the model. \n",
        "\n",
        "This same approach can be used with regularized linear models, such as Ridge and ElasticNet. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko_JB14wXLrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "6782b440-fc04-417a-9980-7757fd7b63d2"
      },
      "source": [
        "# linear regression feature importance\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from matplotlib import pyplot as plt \n",
        "# define the model\n",
        "model = LinearRegression()\n",
        "# fit the model\n",
        "model.fit(X_reg, y_reg)\n",
        "# get importance\n",
        "importance = model.coef_\n",
        "# summarize feature importance\n",
        "for i,v in enumerate(importance):\n",
        "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
        "# plot feature importance\n",
        "plt.figure(figsize = (10,6))\n",
        "plt.bar([x for x in range(len(importance))], importance)\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature: 0, Score: 26.41417\n",
            "Feature: 1, Score: -0.00000\n",
            "Feature: 2, Score: 70.90040\n",
            "Feature: 3, Score: 0.00000\n",
            "Feature: 4, Score: 62.16242\n",
            "Feature: 5, Score: -0.00000\n",
            "Feature: 6, Score: -0.00000\n",
            "Feature: 7, Score: 3.66728\n",
            "Feature: 8, Score: 0.00000\n",
            "Feature: 9, Score: 0.00000\n",
            "Feature: 10, Score: 0.00000\n",
            "Feature: 11, Score: -0.00000\n",
            "Feature: 12, Score: 68.99048\n",
            "Feature: 13, Score: 25.57761\n",
            "Feature: 14, Score: 0.00000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di-9BNGJY7mU",
        "colab_type": "text"
      },
      "source": [
        "<ins>**Logistic Regression Feature Importance**</ins>\n",
        "\n",
        "In a similar fashion, we can do the same to fit a [*LogisticRegression()*](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS4mefGrZgei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "6c3bcafb-4128-49cf-ebb6-da21073c8523"
      },
      "source": [
        "# logistic regression for feature importance\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib import pyplot as plt\n",
        "# define the model\n",
        "model = LogisticRegression()\n",
        "# fit the model\n",
        "model.fit(X_clf, y_clf)\n",
        "# get importance\n",
        "importance = model.coef_[0]\n",
        "# summarize feature importance\n",
        "for i,v in enumerate(importance):\n",
        "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
        "# plot feature importance\n",
        "plt.bar([x for x in range(len(importance))], importance)\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature: 0, Score: -0.47207\n",
            "Feature: 1, Score: 0.15415\n",
            "Feature: 2, Score: -0.11580\n",
            "Feature: 3, Score: 0.08169\n",
            "Feature: 4, Score: 0.22694\n",
            "Feature: 5, Score: 0.03438\n",
            "Feature: 6, Score: 0.12977\n",
            "Feature: 7, Score: -0.03683\n",
            "Feature: 8, Score: 0.72793\n",
            "Feature: 9, Score: 0.49874\n",
            "Feature: 10, Score: 0.17051\n",
            "Feature: 11, Score: 0.26644\n",
            "Feature: 12, Score: -0.16955\n",
            "Feature: 13, Score: 0.29474\n",
            "Feature: 14, Score: -0.02333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b32qQh04axsK",
        "colab_type": "text"
      },
      "source": [
        "Recall that this is a classification problem with classes 0 and 1 (binary). \n",
        "\n",
        "*Why can't we analyze a regression problem with Logistic Regression?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfhV-Dr7QHl9",
        "colab_type": "text"
      },
      "source": [
        "<a id='section2.1'></a>\n",
        "### **Feature Extraction**\n",
        "\n",
        "<a id='section2.2'></a>\n",
        "### **Feature Selection**\n",
        "\n",
        "<a id='section2.3'></a>\n",
        "### **Feature Construction**\n",
        "\n",
        "<a id='section3'></a>\n",
        "## **Discerning between different kinds of features**\n",
        "\n",
        "<a id='section4'></a>\n",
        "## **Handling missing values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WJ5zWTYQHl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}