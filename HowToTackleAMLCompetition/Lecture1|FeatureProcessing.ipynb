{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Feature Processing\n",
    "(Author: Simone Azeglio, simone.azeglio@edu.unito.it)\n",
    "\n",
    "\n",
    "* [Practical Gradient Descent](#section1)\n",
    "    * [Macro, Meso, Micro-scale in Science](#section1.1)\n",
    "    * [Scaling in Data Science](#section1.2)\n",
    "    * [Preprocessing Data](#section1.3)\n",
    "* [Feature Engineering](#section2)\n",
    "    * [Feature Importance](#section2.0)\n",
    "    * [Feature Extraction](#section2.1)\n",
    "    * [Feature Selection](#section2.2)\n",
    "    * [Feature Construction](#section2.3)\n",
    "* [Discerning between different kinds of features](#section3)\n",
    "* [Handling missing values](#section4)\n",
    "\n",
    "<a id='section1'></a>\n",
    "## Practical Gradient Descent\n",
    "In this first part, we'd like to tell you about some practical tricks for making **gradient descent** work well, in particular, we're going to delve into feature scaling. As an introductory view, it seems reasonable to try to depict an intuition of the concept of *scale*. \n",
    "\n",
    "<a id='section1.1'></a>\n",
    "### Macro, Meso, Micro-scale in Science\n",
    "\n",
    "As scientists, we are well aware of the effects of using a specific measurement tool in order to characterize some quantity and describe reality. As an ideal example we consider the **length scale**. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sazio/sazio.github.io/master/_posts/img/HowToTackle/Lecture1/1.0.png\" width=\"500\" height=\"300\">\n",
    "\n",
    "We can identify three different points of view: *microscopic*, *mesoscopic* and *macroscopic*; which are intimately related to the adopted lenght scale. \n",
    "\n",
    "We usually deal with the *macroscopic scale* when the observer is in such a position (pretty far, in terms of distance), with respect to the object, that she/he can describe its global characteristics. Instead, we do refer to the *microscopic scale* when the observer is so close to the object that she/he can describe its atomistic details or elementary parts (e.g. molecules, atoms, quarks). Last but not least, we talk about *mesoscopic scale* everytime we are in between micro and macro. \n",
    "\n",
    "These definitions are deliberately vague, since delineating a precise and neat explanation would be higly difficult and complex, and it's actually far from our purposes. \n",
    "\n",
    "On the other side, this kind of introduction is quite useful, we should take a few minutes to think about the \"active\" role of the observer and about the fact that, to be honest, for every length scale, there's some specific theory, i.e. there's no global theory for a multi-scale description of some phenomenon. \n",
    "\n",
    "<a id='section1.2'></a>\n",
    "### Scaling in Data Science\n",
    "\n",
    "If our beloved observer (i.e. the scientist) has some kind of \"privilege\", i.e. choosing the right measurement tool, which is nothing but choosing the right scale in the description of some phenomenon, we can't really say the same for a data scientist. \n",
    "\n",
    "It's a sort of paradox, but a data scientist can't really deal with data retrieval most of the times. Because of that, a data scientist is often left alone in front of data, without even knowing from which measurement tool they're coming from. There's no way to interact with the length scale for example. \n",
    "\n",
    "Is there something that we can do about it? The only thing we can do is assuming that features are independent and scale these features in order to have something compatible from one to the other. This procedure is called **feature scaling**, and soon we'll understand why it is useful even for ML algorithms, such as gradient descent. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sazio/sazio.github.io/master/_posts/img/HowToTackle/Lecture1/1.1.png\" width=\"500\" height=\"300\">\n",
    "\n",
    "If you make sure that features are on similar scales, i.e. features take on similar range of values, then gradient descent can converge more quickly. \n",
    "\n",
    "More concretely, let's say we have a problem with two features where $x_1$ is the length of a football field and take values between $90$ (meters) and $115$ (meters) and $x_2$ is the radius of a ball which takes values between $10.5* 10^{-2}$ (meters) to $11.5* 10^{-2}$ (meters). If you plot the countours of the cost function $J(\\omega)$ then you might get something similar to the *left plot*, and because of these very skewed elliptical shape, if we run gradient descent on this cost function, it may end up taking a long time and oscillating back and forth before reaching the global minimum. \n",
    "\n",
    "In these settings, as stated previously, a useful thing to do is to scale the features. Generally, the idea is to get every feature into approximately a $-1$ to $+1$ range. By doing this, we get the *right plot*. In this way, you can find a much more direct path to the global minimum rather than taking a much more convoluted path where you're sort of trying to follow a very complicated trajectory. \n",
    "\n",
    "<a id='section1.3'></a>\n",
    "### Preprocessing Data\n",
    "ToDo: \n",
    "\n",
    "CODE\n",
    "\n",
    "\n",
    "\n",
    "<a id='section2'></a>\n",
    "## Feature Engineering\n",
    "\n",
    "The real deal is that nobody explicitly tells you what **feature engineering** is, in some way, you are expected to understand for yourself what are good features.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sazio/sazio.github.io/master/_posts/img/HowToTackle/Lecture1/1.2.jpg\" width=\"700\" height=\"400\">\n",
    "\n",
    "> Feature engineering is another topic which doesn’t seem to merit any review papers or books, or even chapters in books, but it is absolutely vital to ML success. […] Much of the success of machine learning is actually success in engineering features that a learner can understand. \n",
    "\n",
    ">*(Scott Locklin, in “Neglected machine learning ideas”)* \n",
    "\n",
    "Let's try to figure out what feature engineering is. \n",
    "\n",
    "In solving such problems, our goal is to get the best possible result from a model. In order to achieve that, we need to extract useful information and get the most from what we have. On one side, this includes getting the best possible result from the algorithms we are employing. On the other side, it also involves getting the most out of the available data. \n",
    "\n",
    "*How do we get the most out of our data for predictive modeling?* \n",
    "\n",
    "Feature engineering tries to find an answer to this question. \n",
    "\n",
    "> Actually, the success of all Machine Learning algorithms depends on how you present the data. \n",
    "\n",
    "> (*Mohammad Pezeshki, answer to “What are some general tips on feature selection and engineering that every data scientist should know?\"*)\n",
    "\n",
    "\n",
    "\n",
    "<a id='section2.0'></a>\n",
    "### Feature Importance\n",
    "Feature importance refers to a bunch of techniques that assign a score to input features based on how useful they are at predicting a target variable. These scores play an important role in predictive modeling, they usually provide useful insights into the dataset and the basis for dimensionality reduction and feature selection. \n",
    "\n",
    "Feature importance scores can be calculated both for regression and classification problems. \n",
    "\n",
    "These scores can be used in a range of situations, such as:\n",
    "* *Better understanding the data*: the relative scorse can highlight which features may be most relevant to the target, and on the other side, which are least relevant. This could be a useful notion for a domain expert and could be used as a basis for gathering more or different data. \n",
    "\n",
    "\n",
    "* *Better understanding a model*: inspecting the importance score provides insights into the specific model we're using and which features are the most important to the model when elaborating a prediction. \n",
    "\n",
    "\n",
    "* *Reducing the number of input features*: we can use the importance scores to select those features to delete (lowest score) and those to keep (highest scores). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b7c74cbf5af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2.1'></a>\n",
    "### Feature Extraction\n",
    "\n",
    "<a id='section2.2'></a>\n",
    "### Feature Selection\n",
    "\n",
    "<a id='section2.3'></a>\n",
    "### Feature Construction\n",
    "\n",
    "<a id='section3'></a>\n",
    "## Discerning between different kinds of features\n",
    "\n",
    "<a id='section4'></a>\n",
    "## Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
